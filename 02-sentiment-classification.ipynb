{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your sentiment classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![image](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hieultp/holistics-nlp-workshop/blob/main/02-sentiment-classification.ipynb)\n",
    "\n",
    "Good to see you again ;) Let continue our journey then.\n",
    "\n",
    "Just in case you forgot what we need to train your ~dragon~ AI model ðŸ‘€:\n",
    "- Prepare data\n",
    "- Define the model\n",
    "- Define the loss function\n",
    "- Train it!\n",
    "\n",
    "Okay, let's dive in then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/hieultp/holistics-nlp-workshop\n",
    "%cd holistics-nlp-workshop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's us prepare the dataset. We will use the TweetEval dataset this time.\n",
    "\n",
    "The dataset contains many tweets and sentiment labels associate with each tweet.\n",
    "\n",
    "Usually, we will have three seperated image sets. The first one is to used for training the model (train set), the second is used for validating (validation set), and the last one will be used for testing (test set).\n",
    "\n",
    "The reason we need an additional set, validation set, is that we will tune our model based on this one. Avoiding checking the test set too many times, which might lead to overfiting to the test set.\n",
    "\n",
    "But for simplicity, let's settle at two dataset only now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torchtext==0.14.1\n",
    "\n",
    "from src.datasets import TweetEvalSetiment\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional step before building the vocabulary is that we have to tokenize the input text.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*UhfwmhMN9sdfcWIbO5_tGg.jpeg\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAPPING = {\n",
    "    0: \"negative\",\n",
    "    1: \"neutral\",\n",
    "    2: \"positive\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TweetEvalSetiment(type=\"train\")\n",
    "test_data = TweetEvalSetiment(type=\"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a sample from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, label = train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAPPING[label]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good enough. Let's build our vocabulary then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator((text for text, _ in train_data), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could notice that we have two additional tokens: `<unk>` and `<pad>`.\n",
    "\n",
    "The `<unk>` token will be used in the inference when we encounters a new word that we don't have in the vocabulary. `<pad>` token will be used when we prepared the dataset for our model.\n",
    "\n",
    "Usually a DL model operates on a fixed length sentence. Thus we will pad each sentence to a fixed length, and truncate those longer than our predefined length.\n",
    "\n",
    "Note: You can also play around with the tokenizer. Different ways of tokenizing results in different vocabulary and ultimately affects the model performance too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "MAX_SEQUENCE_LEN = max(len(text_pipeline(text)) for text, _ in train_data)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text_list, label_list = [torch.zeros(MAX_SEQUENCE_LEN, dtype=torch.int64)], []\n",
    "    for _text, _label in batch:\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        label_list.append(_label)\n",
    "    text_list = torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])[1:]\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    return text_list, label_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to define our model. This model simply contains an embedding followed by a linear layer.\n",
    "\n",
    "You could play around with the embedding size, or take a look a the source code and change the number of linear layer then. ðŸ‘€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import TextClassificationModel\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "model = TextClassificationModel(\n",
    "    vocab_size, embed_dim=64, num_class=len(LABEL_MAPPING), padding_idx=vocab[\"<pad>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, let's define the loss function and we're ready to go."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just like the previous excercise. Take your time to explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# loss_fn = nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's timeeeee."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://i.imgflip.com/3f23r3.jpg\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import train\n",
    "\n",
    "model = train(model, loss_fn, train_data, test_data, num_epochs=10, collate_fn=collate_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that you have your model trained (and hopefully the performance on the test set does no bad).\n",
    "\n",
    "Let's test it. Run the three cells below and draw any numbers then see if your model could guess it or not.\n",
    "\n",
    "Then feel free to go back and play around with the loss function, the model to see whether you could improve the model performance or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gradio\n",
    "\n",
    "import torch\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text):\n",
    "    text = torch.nn.utils.rnn.pad_sequence(\n",
    "        [\n",
    "            torch.zeros(MAX_SEQUENCE_LEN, dtype=torch.int64),\n",
    "            torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        ],\n",
    "        batch_first=True,\n",
    "        padding_value=vocab[\"<pad>\"]\n",
    "    )[1:].to(device)\n",
    "    prediction = model(text).softmax(1).squeeze()\n",
    "    return {LABEL_MAPPING[i]: prediction[i].item() for i in range(3)}\n",
    "\n",
    "gr.Interface(\n",
    "    fn=inference,\n",
    "    inputs=\"text\",\n",
    "    outputs=gr.outputs.Label(num_top_classes=3),\n",
    "    live=True,\n",
    "    css=\".footer {display:none !important}\",\n",
    "    title=\"Sentiment Analysis\",\n",
    "    description=\"Enter a tweet and see predictions in real time.\",\n",
    "    thumbnail=\"https://raw.githubusercontent.com/gradio-app/real-time-mnist/master/thumbnail2.png\"\n",
    ").launch()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use a pretrained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model on our own sometimes can be painful ðŸ¥² Let's try some other pretrained model that available freely on the internet thanks to dedicated researchers and practicioners.\n",
    "\n",
    "A very popular community for sharing NLP models is `ðŸ¤— Hugging Face`. We will use their library and models available freely their by contributors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers emoji\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pretrained_model = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_inference(text):\n",
    "    return {\n",
    "        \"NEG\": \"Negative\",\n",
    "        \"NEU\": \"Neutral\",\n",
    "        \"POS\": \"Positive\",\n",
    "    }[pretrained_model(text)[0]['label']]\n",
    "\n",
    "gr.Interface(\n",
    "    fn=pretrained_inference,\n",
    "    inputs=\"text\",\n",
    "    outputs=gr.outputs.Textbox(),\n",
    "    live=True,\n",
    "    css=\".footer {display:none !important}\",\n",
    "    title=\"Sentiment Analysis\",\n",
    "    description=\"Enter a tweet and see predictions in real time.\",\n",
    "    thumbnail=\"https://raw.githubusercontent.com/gradio-app/real-time-mnist/master/thumbnail2.png\"\n",
    ").launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite convenient right? If you happens to have any cool models, don't hesitate to share it back with the commnunity then ;)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for this notebook. Thank you for staying until this end ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holistics-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fa40b19fd51f49cc48f840071f184241e37c8818c02d68d6f8318fbec6f7c32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
